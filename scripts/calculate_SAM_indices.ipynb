{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook processes and analyzes daily and monthly zonal wind data (ua) from Atmospheric Model Intercomparison Project (AMIP) simulations to compute the Southern Annular Mode (SAM) index across multiple climate models. Daily SAM index will be then used to calculate SAM persistence timescale in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jet_latitude(file_name, level):\n",
    "    \"\"\"\n",
    "    Calculate the daily latitude of the jet stream for a given pressure level \n",
    "    from a NetCDF dataset.\n",
    "\n",
    "    Parameters:\n",
    "    file_name : str\n",
    "        Path to NetCDF file containing wind data.\n",
    "    level : int\n",
    "        Index of the pressure level in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    jet_lat_pfj_daily_zonal_mean : pandas.Series\n",
    "        Daily jet latitudes indexed by time.\n",
    "    \"\"\"\n",
    "    # Load NetCDF dataset using xarray\n",
    "    dataset = xr.open_dataset(file_name, decode_cf=True)\n",
    "\n",
    "    # Select zonal wind data at the specified pressure level\n",
    "    zonal_wind = dataset['ua'].isel(plev=level)\n",
    "\n",
    "    # Compute daily zonal mean wind between -65째 and -20째 latitude\n",
    "    ua_pfj_zonal_mean = zonal_wind.sel(lat=slice(-65, -20)).mean(dim='lon')\n",
    "\n",
    "    # Get number of time steps (days)\n",
    "    numdays = len(dataset.variables['time'][:])\n",
    "\n",
    "    # Initialize arrays to store daily jet latitude and wind speed\n",
    "    arrays = [np.full(numdays, np.nan) for _ in range(2)]\n",
    "    jet_lat_pfj_daily_zonal_mean, jet_speed_pfj_daily_zonal_mean = arrays\n",
    "\n",
    "    # Loop over each day and calculate the jet latitude and strength\n",
    "    for i in range(numdays):\n",
    "        jet_lat_pfj_daily_zonal_mean[i], jet_speed_pfj_daily_zonal_mean[i] = find_jet_lat(\n",
    "            ua_pfj_zonal_mean[i, :], ua_pfj_zonal_mean.lat.values\n",
    "        )\n",
    "\n",
    "    # Return the jet latitude series with time index\n",
    "    jet_lat_pfj_daily_zonal_mean = pd.Series(jet_lat_pfj_daily_zonal_mean, index=dataset.variables['time'][:])\n",
    "    return jet_lat_pfj_daily_zonal_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sam_models(file):\n",
    "    \"\"\"\n",
    "    Calculate the leading principal component (PC) time series of zonal wind anomalies\n",
    "    over the Southern Hemisphere mid-to-high latitudes for CMIP6 model outputs, often used as a proxy \n",
    "    for the Southern Annular Mode (SAM).\n",
    "\n",
    "    Parameters:\n",
    "    file : str\n",
    "        Path to the NetCDF file containing 'ua' (zonal wind) data.\n",
    "\n",
    "    Returns:\n",
    "    leading_pc_timeseries : pandas.Series\n",
    "        Time series of the leading principal component of the zonal wind anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load dataset and subset the zonal wind (ua) field\n",
    "    # Select latitudes between -70째 and -20째 and pressure levels between 1000 hPa and 200 hPa\n",
    "    ua_subset = xr.open_dataset(file)['ua'].sel(lat=slice(-70, -20), plev=slice(100000, 20000)).mean('lon')\n",
    "    print('read in done')\n",
    "\n",
    "    # 2. Remove the seasonal cycle (de-seasonalize)\n",
    "    # Compute the climatological daily mean (seasonal cycle) by grouping by day of year\n",
    "    climatology = ua_subset.groupby(\"time.dayofyear\").mean(\"time\")\n",
    "\n",
    "    # Subtract the climatology from the data to get daily anomalies\n",
    "    ua_anomalies = ua_subset.groupby(\"time.dayofyear\") - climatology\n",
    "\n",
    "    # 3. Flatten the 2D fields (plev x lat) into a 1D feature vector per time step for PCA\n",
    "    ua_reshaped = ua_anomalies.stack(features=(\"plev\", \"lat\")).fillna(0)\n",
    "    print(ua_reshaped.shape)\n",
    "\n",
    "    # 4. Apply Principal Component Analysis (PCA)\n",
    "    # Keep only the leading component (most dominant mode of variability)\n",
    "    pca = PCA(n_components=1)\n",
    "    leading_pc = pca.fit_transform(ua_reshaped)\n",
    "\n",
    "    # 5. Convert the leading PC into a pandas Series with time index\n",
    "    leading_pc_timeseries = pd.Series(leading_pc[:, 0], index=ua_subset.time.values)\n",
    "\n",
    "    return leading_pc_timeseries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating daily SAM indices for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_models_daily = {}\n",
    "#directory = 'directory that contains daily ua from amip runs for all models'\n",
    "directory = '/OWC/huiyu/CMIP6/ua/amip_daily/combined'\n",
    "file_list = os.listdir(directory)\n",
    "for file in file_list:\n",
    "    file_name = os.path.join(directory, file)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    model_name = file.split('_')[2]\n",
    "    if file_name:\n",
    "        sam_models_daily[model_name] = calculate_sam_models(file_name)\n",
    "        if isinstance(sam_models_daily[model_name].index, pd.DatetimeIndex):\n",
    "            print(\"The index is a pandas DatetimeIndex.\")\n",
    "        else:\n",
    "            sam_models_daily[model_name].index = cftime_to_datetime(sam_models_daily[model_name].index)\n",
    "            print(\"The index is not a pandas DatetimeIndex.\")\n",
    "\n",
    "        sam_models_daily[model_name] = sam_models_daily[model_name].loc['2000':'2014']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating monthly SAM indices for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_models_monthly = {}\n",
    "climatological_jet_lat_models_monthly = {}\n",
    "#directory = 'directory that contains monthly ua from amip runs for all models'\n",
    "directory = '/OWC/huiyu/CMIP6/ua/amip_monthly/combined'\n",
    "file_list = os.listdir(directory)\n",
    "for file in file_list:\n",
    "    if file.startswith('ua'):\n",
    "        file_name = os.path.join(directory, file)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        model_name = file.split('_')[2]\n",
    "        if file_name:\n",
    "            sam_models_monthly[model_name] = calculate_sam_models(file_name)\n",
    "            jet_lat = calculate_jet_latitude(file_name, 2) # 850hPa\n",
    "            if isinstance(sam_models_monthly[model_name].index, pd.DatetimeIndex):\n",
    "                print(\"The index is a pandas DatetimeIndex.\")\n",
    "            else:\n",
    "                sam_models_monthly[model_name].index = cftime_to_datetime(sam_models_monthly[model_name].index)\n",
    "                jet_lat.index = cftime_to_datetime(jet_lat.index)\n",
    "                print(\"The index is not a pandas DatetimeIndex.\")\n",
    "\n",
    "            sam_models_monthly[model_name] = sam_models_monthly[model_name].loc['2000':'2014']\n",
    "            climatological_jet_lat_models_monthly[model_name] = jet_lat.loc['2000':'2014'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = '../data'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the dictionaries using pickle\n",
    "#with open(os.path.join(save_directory, 'sam_models.pkl'), 'wb') as f:\n",
    "    #pickle.dump(sam_models_daily, f)\n",
    "\n",
    "with open(os.path.join(save_directory, 'sam_models_monthly_2000_2014.pkl'), 'wb') as f:\n",
    "    pickle.dump(sam_models_monthly, f)\n",
    "\n",
    "with open(os.path.join(save_directory, 'climatological_jet_lat_models_monthly_2000_2014.pkl'), 'wb') as f:\n",
    "    pickle.dump(climatological_jet_lat_models_monthly, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isca_env",
   "language": "python",
   "name": "isca_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
